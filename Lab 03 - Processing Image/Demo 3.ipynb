{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03: Processing Images with OpenCV - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting images between different color models\n",
    "\n",
    "We will be working with three kinds of color models:\n",
    "\n",
    "*   **Grayscale** is a model that reduces color information by translating it into shades\n",
    "of gray or brightness. \n",
    "\n",
    "*   **BGR** is the blue-green-red color model, in which each pixel has a triplet of values\n",
    "representing the blue, green, and red components or channels of the pixel's color.\n",
    "\n",
    "*   **HSV** model uses a different triplet of channels. Hue is the color's tone,\n",
    "saturation is its intensity, and value represents its brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default BGR image\n",
    "img = cv2.imread('./Images/child.bmp')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert BGR to RGB\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to grayscale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to HSV\n",
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "plt.imshow(img_hsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Fourier transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fourier transform allows us to identify regions in images where a signal (such as the values of image pixels) changes a lot, and also regions where the change is less dramatic.\n",
    "\n",
    "*   The Fourier transform gives information about the frequency content of the image.\n",
    "    *  It is used to decompose an image into its sine and cosine components.\n",
    "*   It can be used for image enhancement, analysis, restoration, compression, edge detection, and shape detection.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Read the image as a gray scale: `i = cv2.imread(image path, 0)`\n",
    "\n",
    "2.  Convert the image to float: `img_f = np.float32(i)`\n",
    "\n",
    "3.  Apply Discrete Fourier Transform: `dft = cv2.dft(img_f, cv2.DFT_COMPLEX_OUTPUT)`\n",
    "    *   It returns complex output (real numbers, imaginary numbers) <br>\n",
    "    <br>\n",
    "\n",
    "4.  Shift the lightness of the image to the center of the image: `dft_shift = np.fft.fftshift(dft)`\n",
    "\n",
    "5.  Compute the amount of change: `res = 20*np.log(cv2.magnitude(dft_shift[:,:],dft_shift[:,:]))`\n",
    "    * The function `cv2.magnitude()` calculates the magnitude of the `dft_shift` real vector and imaginary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution operation\n",
    "\n",
    "![convolution operation](./data/2D_Convolution_Animation.gif)\n",
    "\n",
    "src: https://commons.wikimedia.org/wiki/File:2D_Convolution_Animation.gif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Low-Pass Filter\n",
    "\n",
    "LPF is the type of frequency domain filter that is used for smoothing the image. It attenuates the high frequency components and preserves the low frequency components.\n",
    "\n",
    "One of the most popular blurring/smoothening filters, the Gaussian blur, is a low-pass filter that\n",
    "attenuates the intensity of high-frequency signals.\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image\n",
    "\n",
    "2.  Apply Gaussian blur to the image: `cv2.GaussianBlur(image, kernel size, deviation of the filter)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying High Pass Filter\n",
    "\n",
    "HPF is the type of frequency domain filter that is used for sharpening the image. It attenuates the low frequency components and preserves the high frequency components.\n",
    "\n",
    "\n",
    "HPF-image can be generated by subtracting a gaussian blurred image (low-pass filtered image) from the original image.\n",
    "\n",
    "**Steps**\n",
    "1.  Subtract LP filtered image from the original image: `img_hpf = img - cv2.GaussianBlur(img, (17, 17), 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-pass filtered image + low-pass filtered image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying High-boost Filter\n",
    "\n",
    "The high boost filter is a typed of high-boost filter that is  used to enhance the high frequency components without eliminating low frequency components.\n",
    "\n",
    "For a 3x3 kernel, the high-boost filter is\n",
    "\n",
    "|    |    |    |\n",
    "|----|----|----|\n",
    "| -1 | -1 | -1 |\n",
    "| -1 | 8  | -1 |\n",
    "| -1  | -1 | -1 |\n",
    "\n",
    "\n",
    "For a 5x5 kernel\n",
    "\n",
    "|    |    |    |    |    |\n",
    "|----|----|----|----|----|\n",
    "| -1 | -1 | -1 | -1 | -1 |\n",
    "| -1 | 1  | 2  | 1  | -1 |\n",
    "| -1 | 2  | 4  | 2  | -1 |\n",
    "| -1 | 1  | 2  | 1  | -1 |\n",
    "| -1 | -1 | -1 | -1 | -1 |\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Define the kernel:\n",
    "    * For 3*3 kernel:\n",
    "            $\\left[\\begin{array}{ccc}\n",
    "                -1 & -1 & -1\\\\\n",
    "                -1 & 8 & -1\\\\\n",
    "                -1 & -1 & -1\n",
    "                \\end{array}\\right]$\n",
    "    \n",
    "    * For 5*5 kernel:\n",
    "            $\\left[\\begin{array}{ccc}\n",
    "                -1 & -1 & -1 & -1 & -1\\\\,\n",
    "                -1 & 1 & 2 & 1 & -1 \\\\,\n",
    "                -1 & 2 & 4 & 2 & -1\\\\,\n",
    "                -1 & 1 & 2 & 1 & -1\\\\,\n",
    "                -1 & -1 & -1 & -1 & -1 \n",
    "                \\end{array}\\right]$\n",
    "\n",
    "3. Apply the filter to the image using `convolve` function in scipy:  \n",
    "    *   `img_k3 = ndimage.convolve(img, k3)`\n",
    "    *   `img_k5 = ndimage.convolve(img, k5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `cv2.filter2D` to apply filters to images.\n",
    "\n",
    "`filter2D (src, ddepth, kernel)`\n",
    "\n",
    "`Ddepth` â€“ Depth of the output image (Depth is the \"precision\" of each pixel. Typically it can be 8/24/32 bit for displaying) [ -1 will give the output image depth as same as the input image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Let's imagine we want to detect the edges present in the image. For instance:\n",
    "\n",
    "![Image](./data/edge.jpg)\n",
    "\n",
    "*   You can easily notice that in an edge, the pixel intensity changes in a notorious way. \n",
    "\n",
    "*   A good way to express changes is by using derivatives. A high change in gradient indicates a major change in the image.\n",
    "\n",
    "*   Let's assume we have a 1D-image. An edge is shown by the \"jump\" in intensity in the plot below:\n",
    "\n",
    "![Image](./data/edg1.jpg)\n",
    "\n",
    "*   The edge \"jump\" can be seen more easily if we take the ***first derivative***.\n",
    "\n",
    "![Image](./data/edg2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel Operator\n",
    "\n",
    "*   Horizontal edge detection\n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "-1 & 0 & +1\\\\\n",
    "-2 & 0 & +2\\\\\n",
    "-1 & 0 & +1 \\\\\n",
    "\\end{array}\\right]$\n",
    "\n",
    "*   Vertical edge detection \n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "-1 & -2 & -1\\\\\n",
    "0 & 0 & 0\\\\\n",
    "+1 & +2 & +1 \\\\\n",
    "\\end{array}\\right]$\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Read the image as gray scale.\n",
    "\n",
    "2.  Apply Sobel filter: `cv2.Soble(src image, ddepth, x_order, y_order, kernel size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Operator\n",
    "\n",
    "*   Laplacian operator applies the second derivative to images to detect edges.\n",
    "\n",
    "![](./data/edg3.jpg)\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Read the image as gray scale\n",
    "\n",
    "2.  Apply the Laplacian filter: `cv2.Laplacian(src_gray, ddepth, ksize=kernel_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to blur the image before detecting edges to avoid detecting noise.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Read the image as gray scale: `img = cv2.imread(image path, 0)`\n",
    "\n",
    "2.  Apply median blur filter: `img_blur = cv2.medianBlur(img, ksize)`\n",
    "\n",
    "3.  Apply Sobel filter in x, y, x and y direction: \n",
    "    *   `img_blur_soble = cv2.Sobel(img_blur, -1, 1, 1, (7, 7))`\n",
    "    *   `img_blur_soble_x = cv2.Sobel(img_blur, -1, 1, 0, (7, 7))`\n",
    "    *   `img_blur_soble_y = cv2.Sobel(img_blur, -1, 0, 1, (7, 7))` <br>\n",
    "    <br>\n",
    "\n",
    "4.  Apply Laplacian filter: `img_blur_lap = cv2.Laplacian(img_blur, -1, (7, 7))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny edge detection\n",
    "\n",
    "The Canny edge detection algorithm is a five-step process:\n",
    "\n",
    "1. Denoise the image with a Gaussian filter.\n",
    "\n",
    "2. Calculate the gradients.\n",
    "\n",
    "3. Apply non-maximum suppression (NMS) on the edges. Basically, this means that the algorithm selects the best edges from a set of overlapping edges.\n",
    "\n",
    "4. Apply a double threshold to all the detected edges to eliminate any false positives.\n",
    "\n",
    "5. Analyze all the edges and their connection to each other to keep the real edges and discard the weak ones.\n",
    "\n",
    "`cv2.Canny(image, threshold1, threshold2, apertureSize, L2gradient)`\n",
    "1.  `image`: Source/Input image of n-dimensional array.\n",
    "\n",
    "2.  `threshold1`: It is the low threshold value of intensity gradient.\n",
    "\n",
    "3.  `threshold2`: It is the high threshold value of intensity gradient.\n",
    "\n",
    "**Optional parameters are:**\n",
    "\n",
    "4.  `apertureSize`: Order of Kernel(matrix) for the Sobel filter. Its default value is (3 x 3), and its value should be odd between 3 and 7. It is used for finding image gradients. Filter is used for smoothening and sharpening of an image.\n",
    "\n",
    "5.  `L2gradient`: This specifies the equation for finding gradient magnitude. L2gradient is of boolean type, and its default value is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour detection\n",
    "\n",
    "We want to detect contours or outlines of subjects contained in an image or video frame -- **Regions of Interest**.\n",
    "\n",
    "**Steps**\n",
    "1.  Create a black image: `img = np.zeros(size, dtype=np.uint8)`\n",
    "\n",
    "2.  Create a white rectangle in the image: `img[50:150, 50:150] = 255`\n",
    "\n",
    "3.  Detect the contours in the image: `cv2.findContours(src image, retrieval mode, approximation mode)`.\n",
    "    *   It returns a list of contours and their hierarchy in the image.\n",
    "\n",
    "    *   `retrieval mode` is an algorithm to find contour within the image.\n",
    "\n",
    "    *   `approximation mode` is an algorithm for reducing the number of points in a curve with a reduced set of points<br>\n",
    "    <br>\n",
    "\n",
    "4.  Convert the gray image to color to see the contours: `color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)`\n",
    "5.  Draw contours using `cv2.drawContours(img, contour list, index of contours, color, thickness)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding contours in irregular, skewed, and rotated shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the bounding box of the subject\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Convert the image to binary image using `cv2.threshold(grayscale image, threshold value, max value, threshold style)`\n",
    "\n",
    "4.  Detect the contours using `cv2.findContours(binary image, retrieval mode, approximation mode)`\n",
    "\n",
    "\n",
    "5.  For each detected contour\n",
    "\n",
    "    1.  Detect the bounding rectangle using `x, y, width, height = cv2.boundingRect(contour)`.\n",
    "\n",
    "    2.  Draw the bounding rectangle using `cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the minimum enclosing rectangle of the subject\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Convert the image to binary image using `cv2.threshold(grayscale image, threshold value, max value, threshold style)`\n",
    "\n",
    "4.  Detect the contours using `cv2.findContours(binary image, retrieval mode, approximation mode)`\n",
    "\n",
    "5.  For each detected contour\n",
    "\n",
    "    1.  Find the minimum area rectangle using `rect = cv2.minAreaRect(contour)`.\n",
    "\n",
    "    2.  Calculate coordinates of the minimum area rectangle using `box = cv2.boxPoints(rect)`.\n",
    "\n",
    "    3.  Normalize coordinates to integers using `box = np.int0(box)`\n",
    "    \n",
    "    4.  Draw contours using `cv2.drawContours(img, [box], 0, (0,0, 255), 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the minimum enclosing circle\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Convert the image to binary image using `cv2.threshold(grayscale image, threshold value, max value, threshold style)`\n",
    "\n",
    "4.  Detect the contours using `cv2.findContours(binary image, retrieval mode, approximation mode)`\n",
    "\n",
    "5.  For each detected contour\n",
    "\n",
    "    1.  Calculate center and radius of minimum enclosing circle: `(x, y), radius = cv2.minEnclosingCircle(c)`\n",
    "    \n",
    "    2.  Cast to integers: `center = (int(x), int(y))` and `radius = int(radius)`\n",
    "\n",
    "    3.  Draw the circle: `img = cv2.circle(img, center, radius, (0, 255, 0), 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the contours of the subject itself\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Convert the image to binary image using `cv2.threshold(grayscale image, threshold value, max value, threshold style)`\n",
    "\n",
    "4.  Detect the contours using `cv2.findContours(binary image, retrieval mode, approximation mode)`\n",
    "\n",
    "5. Draw the contours using `cv2.drawContours(src image, contours list, contour index, color, thickness)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing contours on a black image\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Convert the image to binary image using `cv2.threshold(grayscale image, threshold value, max value, threshold style)`\n",
    "\n",
    "4.  Detect the contours using `cv2.findContours(binary image, retrieval mode, approximation mode)`\n",
    "\n",
    "5.  Create a black image similar to the image itself: `np.zeros_like(img)`\n",
    "\n",
    "6.  Draw the contour on the black image: `cv2.drawContours(src image, contours list, contour index, color, thickness)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate bounding polygon of a shape\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Convert the image to binary image using `cv2.threshold(grayscale image, threshold value, max value, threshold style)`\n",
    "\n",
    "4.  Detect the contours using `cv2.findContours(binary image, retrieval mode, approximation mode)`\n",
    "\n",
    "5.  Create a black image similar to the image itself: `np.zeros_like(img)`\n",
    "\n",
    "6.  For each detected contour:\n",
    "\n",
    "    1.  Compute epsilon, which is the the maximum difference between the original contour and the approximated polygon: \n",
    "    \n",
    "        `epsilon = ratio * cv2.arcLength(contour, closed polygon?)`\n",
    "\n",
    "        `cv2.arcLength`: Calculates a contour perimeter or a curve length\n",
    "\n",
    "\n",
    "    2.  Compute the approximated polygon: `approx = cv2.approxPolyDP(contour,epsilon,closed polygon?)`\n",
    "\n",
    "    3.  Draw the approximated bounding polygon on the black image: `cv2.drawContours(src image, [approx], contour index, color, thickness)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect convex hull\n",
    "\n",
    "Here, `cv2.convexHull()` function checks a curve for convexity defects and corrects it.\n",
    "\n",
    "<img src=\"./data/convx.png\"  width=\"300\" height=\"300\">\n",
    "\n",
    "**Steps**\n",
    "1.  Read the image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Convert the image to binary image using `cv2.threshold(grayscale image, threshold value, max value, threshold style)`\n",
    "\n",
    "4.  Detect the contours using `cv2.findContours(binary image, retrieval mode, approximation mode)`\n",
    "\n",
    "5.  Create a black image similar to the image itself: `np.zeros_like(img)`\n",
    "\n",
    "6.  For each detected contour:\n",
    "\n",
    "    1.  Compute the convex hull: `hull = cv2.convexHull(contour)`\n",
    "\n",
    "    2.  Draw the contours on the black image: `cv2.drawContours(src image, hull, contour index, color, thickness)`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting lines\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Read image: `img = cv2.imread(path to image)`\n",
    "\n",
    "2.  Convert image to gray scale: `img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)`\n",
    "\n",
    "3.  Detect the edges using Canny: `edges = cv2.Canny(image gray, min threshold, max threshold)`\n",
    "\n",
    "4.  Detect lines using HoughLines function: `lines = cv2.HoughLinesP(edges of image, rho, theta, threshold, minLineLen, maxLineGap)`\n",
    "\n",
    "    1.  `rho` is the positional step size in pixels, while `theta` is the rotational step size in radians. For example,\n",
    "        if we specify `rho=1` and `theta=np.pi/180.0`, we search for lines that are separated by as little as 1 pixel and 1 degree.\n",
    "    \n",
    "    2.  `threshold` represents the threshold below which a line is discarded.\n",
    "\n",
    "    3.  `minLineLen` minimum line length to search for.\n",
    "\n",
    "    4.  `maxLineGap` space between the lines in the image.\n",
    "\n",
    "5.  For each line detected:\n",
    "\n",
    "    1.  Decompose line endpoints: `x1, y1, x2, y2 = line[0, 0], line[0, 1], line[0, 2], line[0, 3]`\n",
    "    \n",
    "    2.  Draw a line on the image: `cv2.line(img, (x1, y1), (x2, y2), color, thickness)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting circles\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Read image: `img = cv2.imread(path to image)`\n",
    "\n",
    "2.  Convert image to gray scale: `img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)`\n",
    "\n",
    "3.  Blur the image using median blur filter: `img_gray_blur = cv2.medianBlur(img_gray, 5)`\n",
    "\n",
    "4.  Detect lines using HoughCircles function: `circles = cv2.HoughCircles(gray image, method, dp, minDist, param1, param2, minRadius, maxRadius)`\n",
    "\n",
    "    1.  `method`: Defines the method to detect circles in images. We will use `cv2.HOUGH_GRADIENT`.\n",
    "\n",
    "    2.  `dp`: This parameter is the inverse ratio of the accumulator resolution to the image resolution. [https://stackoverflow.com/questions/33544411/in-the-circle-hough-transform-what-is-the-inverse-ratio-of-accumulator-resoluti]\n",
    "\n",
    "    3.  `minDist`: Minimum distance between the center (x, y) coordinates of detected circles. If the minDist is too small, multiple circles in the same neighborhood as the original may be (falsely) detected. If the minDist is too large, then some circles may not be detected at all.\n",
    "\n",
    "    4.  `param1`: Gradient value used to handle edge detection.\n",
    "\n",
    "    5.  `param2`: Accumulator threshold value for the cv2.HOUGH_GRADIENT method. The smaller the threshold is, the more circles will be detected (including false circles). The larger the threshold is, the more circles will potentially be returned.\n",
    "\n",
    "    6.  `minRadius`: Minimum size of the radius (in pixels).\n",
    "\n",
    "    7.  `maxRadius`: Maximum size of the radius (in pixels).\n",
    "\n",
    "5.  Convert the coordinates of the circles to integer values: `circles = np.uint16(np.around(circles))`\n",
    "\n",
    "6.  For each circle detected:\n",
    "\n",
    "    1.  Decompose line center and radius: `x1, y1, rad = circle[0, 0], c[0, 1], c[0, 2]`\n",
    "    \n",
    "    2.  Draw the outer circle on the image: `cv2.circle(img, (x1, y1), radius, color, thickness)`\n",
    "\n",
    "    3.  Draw the center point on the image: `cv2.circle(img, (x1, y1), radius=2, color, thickness)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "1.  Read the 'stone.jpg' image.\n",
    "\n",
    "2.  Convert the image to gray scale.\n",
    "\n",
    "3.  Apply the following filter to the gray scale image and the colored image.\n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "0 & -1 & 0\\\\\n",
    "-1 & 5 & -1\\\\\n",
    "0 & -1 & 0\n",
    "\\end{array}\\right]$\n",
    "\n",
    "4.  Apply high-pass filter to the image.\n",
    "\n",
    "5.  show the original and the processed images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read an RGB image\n",
    "img = cv2.imread('./Images/stone.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "## Convert to grayscale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "## Define the filter\n",
    "filter = np.array([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n",
    "\n",
    "# filter_rgb = np.vstack([[filter_gray]] * 3)\n",
    "\n",
    "## Apply the filter to grayscale and RGB images\n",
    "img_filter_gray = cv2.filter2D(img_gray, ddepth=-1, kernel=filter)\n",
    "img_filter_rgb = cv2.filter2D(img, ddepth=-1, kernel=filter)\n",
    "\n",
    "## Apply HPF\n",
    "hpf_img = img - cv2.GaussianBlur(img, (9, 9), 0)\n",
    "\n",
    "## show the images\n",
    "imgs = [img, img_gray, img_filter_gray, img_filter_rgb, hpf_img]\n",
    "titles = ['Original Image', 'Grayscale', 'Filter gray', 'Filter RGB', 'HPF']\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\n",
    "for i in range(5):\n",
    "    ax = axs[i]\n",
    "    if i==1 or i==2:\n",
    "        ax.imshow(imgs[i], cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(imgs[i])\n",
    "    \n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Draw the contours, the approximated contours polygon, and convex hull of any image on one black image.\n",
    "\n",
    "Similar to this\n",
    "\n",
    "![](./data/task.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b4ffc9c9a031070cdf645d18822cd1abb711111454341ec96112cbb04136171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
